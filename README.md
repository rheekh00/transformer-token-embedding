# transformer-token-embedding

## 개요

이 저장소는 400만 개의 문장 데이터셋을 기반으로 훈련된 트랜스포머 모델을 사용하여 수행한 토큰 임베딩 실험의 코드와 결과를 포함하고 있습니다. 모델은 Byte Pair Encoding (BPE)을 사용하여 37,000개의 영어 단어로 구성된 어휘를 기반으로 훈련되었습니다. 이 실험은 영어-독일어 및 독일어-영어 번역 작업에 초점을 맞추어 모델의 인코더와 디코더 레이어에서의 토큰 임베딩을 비교합니다.

## 모델 및 훈련 세부 사항

- **모델 아키텍처**: 트랜스포머
- **훈련 데이터**: 400만 개의 문장
- **어휘**: BPE를 사용한 37,000개의 영어 단어
- **훈련 에포크**: 12
- **번역 작업**:
  - 영어 → 독일어
  - 독일어 → 영어

## 실험 세부 사항

이 실험은 모델의 다음 구성 요소에서의 토큰 임베딩을 비교합니다:

1. **인코더 임베딩 레이어의 가중치 (`enc_embed`)**: (37000 * 512)
2. **인코더의 출력 룩업 테이블 (`enc_lut`)**: (37000 * 512)
3. **디코더 임베딩 레이어의 가중치 (`dec_embed`)**: (37000 * 512)
4. **디코더의 출력 룩업 테이블 (`dec_lut`)**: (37000 * 512)

임베딩은 벡터 코사인 유사도와 자카드 유사도를 사용하여 비교되었습니다.

### 임베딩 행렬 공유 실험

이 실험에서는 인코더와 디코더의 임베딩 행렬을 공유했을 때의 성능을 비교했습니다. 일반적으로 트랜스포머 모델에서 인코더와 디코더의 임베딩 행렬을 공유하면 모델의 파라미터 수가 줄어들어 계산 효율성이 향상될 수 있습니다. 그러나 이로 인해 모델의 표현력이 제한될 수도 있습니다.

#### 실험 설정

- **인코더와 디코더 임베딩 행렬 공유**: 인코더와 디코더의 임베딩 행렬을 동일하게 설정하여 모델을 훈련.
- **비교 대상**: 임베딩 행렬을 공유하지 않은 모델과의 성능 비교.

#### 결과

1. **파라미터 수 감소**: 임베딩 행렬을 공유함으로써 모델의 전체 파라미터 수가 약 20% 감소.
2. **성능 변화**:
   - **BLEU 점수**: 임베딩 행렬을 공유한 모델의 BLEU 점수가 약 1.5점 하락.
   - **번역 품질**: 특정 문맥에서의 번역 품질이 약간 저하되었으나, 일반적인 문장에서는 큰 차이가 없음.
3. **학습 속도**: 파라미터 수가 줄어들어 학습 속도가 약 15% 빨라짐.

#### 결론

임베딩 행렬을 공유하면 모델의 파라미터 수가 줄어들어 계산 효율성이 향상되고 학습 속도가 빨라지는 장점이 있습니다. 그러나 이로 인해 모델의 표현력이 약간 제한되어 번역 품질이 약간 하락할 수 있습니다. 따라서, 모델의 크기와 계산 효율성을 고려할 때 임베딩 행렬 공유는 유용한 전략이 될 수 있지만, 번역 품질이 중요한 경우에는 공유하지 않는 것이 더 나을 수 있습니다.

## 결과

### 자카드 유사도

임베딩 간의 자카드 유사도는 다음과 같습니다:

|               | enc_embed | enc_lut | dec_embed | dec_lut |
|---------------|-----------|---------|-----------|---------|
| **enc_embed** | 1         | 0.005   | 0.16      | 0       |
| **enc_lut**   | *         | 1       | 0.002     | 0       |
| **dec_embed** | *         | *       | 1         | 0       |
| **dec_lut**   | *         | *       | *         | 1       |

### 상위 1000개 토큰 쌍의 평균 유사도

1. **인코더 임베딩 레이어의 가중치 (`enc_embed`)**: 0.295
2. **인코더의 출력 룩업 테이블 (`enc_lut`)**: 0.534
3. **디코더 임베딩 레이어의 가중치 (`dec_embed`)**: 0.291
4. **디코더의 출력 룩업 테이블 (`dec_lut`)**: 0.331

### 높은 유사도를 보이는 토큰 쌍

#### Top 500 유사도 토큰쌍
**인코더의 출력 임베딩**
![Image](https://github.com/user-attachments/assets/54d8a819-c7f2-4d14-b658-e0b1281832c1)

**디코더의 출력 임베딩**
![Image](https://github.com/user-attachments/assets/82103961-52cf-4470-a71c-b552200e15ee)

**인코더의 룩업 테이블**
![Image](https://github.com/user-attachments/assets/fd165a5e-2233-4081-9a6f-63fa1d412fff)

**디코더의 룩업 테이블**
![Image](https://github.com/user-attachments/assets/7e49f536-8d56-4934-8c1f-0bdcf2e1a4b2)


#### 인코더의 출력 룩업 테이블

높은 유사도(0.9 이상)를 보이는 토큰들은 특정 문맥에서 자주 함께 등장하는 경우가 많습니다. 예시:

| Token 1      | Token 2      | 유사도 |
|--------------|--------------|--------|
| frame        | work         | 0.846  |
| ob           | tain         | 0.960  |
| estab        | lishing      | 0.948  |
| cry          | stals        | 0.946  |
| contem       | porary       | 0.957  |
| helic        | opters       | 0.933  |

#### 디코더의 출력 룩업 테이블

의미상 유사하거나 문장 내 같은 위치에 등장하는 토큰들이 높은 유사도를 보입니다. 예시:

| Token 1      | Token 2      | 유사도 |
|--------------|--------------|--------|
| require      | requires     | 0.999  |
| effectiveness| efficiency   | 0.887  |
| thereby      | thus         | 0.859  |
| sadly        | unfortunately| 0.701  |
| consumer     | consumers    | 0.667  |

### 임베딩 레이어의 룩업 테이블

#### 인코더 임베딩

| Token 1      | Token 2      | 유사도 |
|--------------|--------------|--------|
| a            | an           | 0.949  |
| especially   | particularly | 0.871  |
| "            | ',           | 0.860  |
| finally      | lastly       | 0.827  |

#### 디코더 임베딩

| Token 1      | Token 2      | 유사도 |
|--------------|--------------|--------|
| adjournment  | resumption   | 0.897  |
| has          | have         | 0.881  |
| especially   | particularly | 0.861  |
| deserve      | deserves     | 0.826  |

## Insights

토큰 쌍(Token-pair)의 상/하위 유사도(similarity) 추이를 통해, 디코더 출력 벡터(decoder output vector)의 표현이 다른 임베딩들과 다른 성격을 가진다는 것을 알 수 있습니다.

- **하위 유사도의 값이 다른 임베딩에 비해 높다.**
- **상위 500쌍의 자카드 유사도 모두 0**
- 다른 임베딩에서는 유사도가 높은 토큰 쌍이 겹치는 경우가 많은 반면, 디코더 아웃풋의 토큰 벡터와는 0에 가까운 유사도를 보인다.

## 결론

이 실험은 의미상 유사하거나 특정 문맥에서 자주 함께 등장하는 토큰들이 높은 코사인 유사도를 보이는 경향이 있음을 보여줍니다. 특히 인코더의 출력 룩업 테이블에서는 특수한 문맥이나 덜 일반적인 문맥에서 함께 등장하는 토큰들이 높은 유사도를 보이는 경우가 많았습니다. 반면, 디코더의 임베딩은 품사나 인칭만 다른 동일한 단어들(예: "make" vs. "makes")이 높은 유사도를 보이는 경우가 많았습니다.

또한, 디코더 출력 벡터의 표현이 다른 임베딩들과는 다른 성격을 가지고 있음을 확인할 수 있었습니다. 특히, 디코더 아웃풋의 토큰 벡터는 다른 임베딩들과는 달리 상위 500쌍의 자카드 유사도가 모두 0에 가까운 값을 보였으며, 하위 유사도의 값이 다른 임베딩에 비해 높게 나타났습니다.
